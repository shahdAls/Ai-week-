{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahdAls/Ai-week-/blob/main/Copy_of_day1_Pytorch_CNN_MNIST_AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n"
      ],
      "metadata": {
        "id": "zwfZGneCdQSg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch torchvision torchsummary\n",
        "%pip install numpy\n",
        "%pip install matplotlib\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "a9FjTEzAdQ28"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tQaUBH8lmnnr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contents\n",
        "\n",
        "In this notebook, we'll once again attempt to train an Auto Encoder model and generate images from it. The images we'll train on and generate are MNIST images (0-9 digits).\n",
        "This time, we'll utilize the power of CNN instead of limiting ourselves to linear layers\n",
        "\n",
        "You need to know:\n",
        "\n",
        "1. **pytorch** (for impelementation)\n",
        "2. a bit of **torch dataloaders and datasets** (not necessary but helps understanding how we're loading data)\n",
        "3. A little bit of **matplotlib** (for result and training trajectory visualization)"
      ],
      "metadata": {
        "id": "gC2jmrtmiUm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                        # transforms.RandomCrop(28, padding=4),\n",
        "                        transforms.ToTensor(),\n",
        "                    ])\n",
        "\n",
        "train_data = MNIST(root='./datasets', train=True, download=True, transform=train_transforms)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "b83RYTZueJCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f93fe3-6eb4-4c29-8265-42088fbe23b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 58539530.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 29843728.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 24564507.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4824139.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNAutoEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, encoding_dim=None):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    self.encoder_block = nn.Sequential(\n",
        "        # input size first time 28\n",
        "        nn.Conv2d(1, 16, 3, 2, 1),\n",
        "        # هنا بس اخذت  ابعاد الصورة في الفرست لير\n",
        "        # 16 filters\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.LeakyReLU(),\n",
        "\n",
        "        nn.Conv2d(16, 32, 3, 2, 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.LeakyReLU(),\n",
        "# layner layer does the learning\n",
        "        nn.Conv2d(32, 64, 3, 2, 1), # kernal size, stried , padding\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.LeakyReLU(),\n",
        "\n",
        "        nn.Flatten(1, -1), # same to reshape(-1,64,4*4) هنا عملت له فلاتن\n",
        "\n",
        "        nn.Linear(64*4*4, 128),\n",
        "        nn.BatchNorm1d(128),\n",
        "        nn.LeakyReLU(),\n",
        "\n",
        "        nn.Linear(128, encoding_dim),\n",
        "        nn.Tanh(),\n",
        "    )\n",
        "#\n",
        "    self.decoder_linear = nn.Sequential(\n",
        "        nn.Linear(encoding_dim, 128),\n",
        "        nn.BatchNorm1d(128),\n",
        "        nn.LeakyReLU(),\n",
        "\n",
        "        nn.Linear(#TODO, #TODO),\n",
        "        nn.BatchNorm1d(#TODO),\n",
        "        nn.LeakyReLU(),\n",
        "    )\n",
        "\n",
        "    self.decoder_cnn = nn.Sequential(\n",
        "\n",
        "        nn.ConvTranspose2d(64, 32, 3,2 ,1),# عكس الماكس بولنق  يكبر\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.LeakyReLU(),\n",
        "\n",
        "        nn.ConvTranspose2d(32, 16, 3, 2, 1),output_padding=1)# hint: the error you're getting is because of a missing argument here.\n",
        "                  # لازم اوتبت بادنق هون ولاحيطلع ايرور\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.LeakyReLU(),\n",
        "\n",
        "        nn.ConvTranspose2d(16, 3, 3, 2, 1, 1),# هنا عدلها من واحد الى 3 لانه من لون قري الى rgb  3\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "  def encoder(self, x):\n",
        "    # Encoder forward pass\n",
        "\n",
        "    x = self.encoder_block(x)\n",
        "    return x\n",
        "\n",
        "  def decoder(self, x):\n",
        "    # Decoder forward pass\n",
        "\n",
        "    x = self.decoder_linear(x)\n",
        "    x = x.reshape((-1, 64, 4, 4))# 2d unfltten convert to 2d ارجعه الى انه يكون\n",
        "        # عشان ادخله على الدكودر لازم ارجعه 2d بهي الطريقه\n",
        "    x = self.decoder_cnn(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "\n",
        "    single_inp = x.ndim == 3.  # 3 dimensions mean [C, H, W] not [B, C, H, W] so the input is a single image not batch\n",
        "\n",
        "    # since the layers expect a batch, let's add a 1 dimension to convert it to [1, C, H, W] where 1 becomes batch size\n",
        "    if single_inp:\n",
        "      x = x.unsqueeze(dim=0)\n",
        "\n",
        "    encoder_out = self.encoder(x)\n",
        "    decoder_out = self.decoder(encoder_out)\n",
        "\n",
        "    # if the input was single images, the outputs should also be the same way for consistency\n",
        "    if single_inp:\n",
        "      encoder_out = encoder_out.squeeze(dim=0)\n",
        "      decoder_out = decoder_out.squeeze(dim=0)\n",
        "\n",
        "    return encoder_out, decoder_out"
      ],
      "metadata": {
        "id": "z1rE51pTmVGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_dim = #TODO\n",
        "model = CNNAutoEncoder(encoding_dim=encoding_dim)"
      ],
      "metadata": {
        "id": "IFKJOAj5Pbdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  summary(model, (1, 28, 28), device='cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWBNQw3LtZd4",
        "outputId": "e89a13a9-4eb8-4300-91ab-6b27ffe99278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 14, 14]             160\n",
            "       BatchNorm2d-2           [-1, 16, 14, 14]              32\n",
            "         LeakyReLU-3           [-1, 16, 14, 14]               0\n",
            "            Conv2d-4             [-1, 32, 7, 7]           4,640\n",
            "       BatchNorm2d-5             [-1, 32, 7, 7]              64\n",
            "         LeakyReLU-6             [-1, 32, 7, 7]               0\n",
            "            Conv2d-7             [-1, 64, 4, 4]          18,496\n",
            "       BatchNorm2d-8             [-1, 64, 4, 4]             128\n",
            "         LeakyReLU-9             [-1, 64, 4, 4]               0\n",
            "          Flatten-10                 [-1, 1024]               0\n",
            "           Linear-11                  [-1, 128]         131,200\n",
            "      BatchNorm1d-12                  [-1, 128]             256\n",
            "        LeakyReLU-13                  [-1, 128]               0\n",
            "           Linear-14                    [-1, 4]             516\n",
            "             Tanh-15                    [-1, 4]               0\n",
            "           Linear-16                  [-1, 128]             640\n",
            "      BatchNorm1d-17                  [-1, 128]             256\n",
            "        LeakyReLU-18                  [-1, 128]               0\n",
            "           Linear-19                 [-1, 1024]         132,096\n",
            "      BatchNorm1d-20                 [-1, 1024]           2,048\n",
            "        LeakyReLU-21                 [-1, 1024]               0\n",
            "  ConvTranspose2d-22             [-1, 32, 7, 7]          18,464\n",
            "      BatchNorm2d-23             [-1, 32, 7, 7]              64\n",
            "        LeakyReLU-24             [-1, 32, 7, 7]               0\n",
            "  ConvTranspose2d-25           [-1, 16, 14, 14]           4,624\n",
            "      BatchNorm2d-26           [-1, 16, 14, 14]              32\n",
            "        LeakyReLU-27           [-1, 16, 14, 14]               0\n",
            "  ConvTranspose2d-28            [-1, 1, 28, 28]             145\n",
            "          Sigmoid-29            [-1, 1, 28, 28]               0\n",
            "================================================================\n",
            "Total params: 313,861\n",
            "Trainable params: 313,861\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.29\n",
            "Params size (MB): 1.20\n",
            "Estimated Total Size (MB): 1.49\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = #TODO\n",
        "lr = #TODO\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "# the best error mean squre error becuse we compare each pixel>\n",
        "criterion = #TODO"
      ],
      "metadata": {
        "id": "tXbOcut4fJFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "\n",
        "for i in range(num_epochs):\n",
        "# هنا ما احتاج الy لانه\n",
        "#unsupervised lersning we donot used labels\n",
        "  epoch_weighted_loss = 0\n",
        "  for (X, _) in train_loader:\n",
        "\n",
        "    X = X.to(device)\n",
        "\n",
        "    (encoder_outs, decoder_outs) = model(X)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(decoder_outs, X)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_weighted_loss += loss.item()*len(X)\n",
        "\n",
        "  epoch_loss = epoch_weighted_loss/len(train_loader.dataset)\n",
        "\n",
        "  print(f'epoch {i+1}/{num_epochs}, loss = {epoch_loss}')\n",
        "\n",
        "  train_losses.append(epoch_loss)"
      ],
      "metadata": {
        "id": "CFXvYta-rxsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b8ea16-cba3-4d84-8a08-80b901560715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/20, loss = 0.11289110354185104\n",
            "epoch 2/20, loss = 0.04920210972030958\n",
            "epoch 3/20, loss = 0.03826261648734411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses)"
      ],
      "metadata": {
        "id": "XUwJuIZosq1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to display results\n",
        "def display_image_grid(images, num_rows, num_cols, title_text):\n",
        "\n",
        "    fig = plt.figure(figsize=(num_cols*3., num_rows*3.), )\n",
        "    grid = ImageGrid(fig, 111, nrows_ncols=(num_rows, num_cols), axes_pad=0.15)\n",
        "\n",
        "    for ax, im in zip(grid, images):\n",
        "        ax.imshow(im, cmap=\"gray\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.suptitle(title_text, fontsize=20)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0wBaVZ0UvkOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will try some actual generation. We will sample the encodings randomly and then pass them through decoder\n",
        "\n",
        "rows, cols = 2, 7\n",
        "sample_encodings = (torch.rand(rows*cols, encoding_dim).to(device) - 0.5) * 2 # encoding space: [-1,1)\n",
        "with torch.no_grad():\n",
        "  generations = model.decoder(sample_encodings).cpu()\n",
        "  generations = generations.reshape(-1, 28, 28, 1)\n",
        "display_image_grid(generations.squeeze(1), rows, cols, \"generated_images\")"
      ],
      "metadata": {
        "id": "aO3WkbTiuQQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXtat3dgXs-9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}